<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Digital Twin for Visual Robot Manipulator</title>
  <!-- Link your main stylesheet -->
  <link rel="stylesheet" href="style.css">
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">
  <style>
    /* Dark theme styling */
    body {
      background-color: #121212;
      font-family: 'Poppins', sans-serif;
      margin: 0;
      padding: 0;
      color: #fff;
    }
    .report-container {
      max-width: 800px;
      margin: 2rem auto;
      padding: 2rem;
      background: #000;
      box-shadow: 0 0 10px rgba(0,0,0,0.8);
      line-height: 1.6;
    }
    .report-container h1, 
    .report-container h2, 
    .report-container h3 {
      color: #1db954;
      text-align: left;
      margin-bottom: 1rem;
    }
    .report-container p, 
    .report-container li, 
    .report-container pre {
      text-align: justify;
      margin-bottom: 1rem;
    }
    .report-container ul, 
    .report-container ol {
      margin-left: 1.5rem;
      margin-bottom: 1rem;
    }
    .report-container figure {
      margin: 1.5rem 0;
      text-align: center;
    }
    .report-container figcaption {
      font-size: 0.9rem;
      color: #aaa;
      margin-top: 0.5rem;
    }
    .video-container {
      text-align: center;
      margin: 1rem 0;
    }
    .video-container img {
      max-width: 100%;
      border-radius: 8px;
    }
    .image-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 1rem;
      margin-bottom: 2rem;
    }
    .image-grid img {
      width: 100%;
      border-radius: 8px;
    }
    .media-link {
      text-align: center;
      margin-top: 2rem;
    }
    .media-link a {
      display: inline-block;
      padding: 0.5rem 1rem;
      background: #1db954;
      color: #fff;
      border-radius: 5px;
      text-decoration: none;
      font-size: 1.2rem;
      transition: background 0.3s ease;
    }
    .media-link a:hover {
      background: #13a04a;
    }
  </style>
</head>
<body>
  <!-- Navigation -->
  <nav>
    <div class="container">
      <a href="index.html" class="logo">Mohammed Ashraf</a>
      <ul class="nav-links">
        <li><a href="index.html#projects">Projects</a></li>
        <li><a href="index.html#experience">Experience</a></li>
        <li><a href="index.html#publications">Publications</a></li>
        <li><a href="index.html#contact">Contact</a></li>
      </ul>
    </div>
  </nav>

  <!-- Report Container -->
  <div class="report-container">
    <h1>Digital Twin Visual Robot Manipulator</h1>
    <h2>A digital twin for a robotic manipulator using Visual Language Action Models</h2>

    
    <!-- Demo Video as YouTube Thumbnail -->
    <!-- <div class="video-container">
      <a href="https://youtu.be/Dy9fhVJdwMo?si=8TiXv9m70lLR1xAU" target="_blank">
        <img src="https://img.youtube.com/vi/Dy9fhVJdwMo/hqdefault.jpg" alt="Demo Video Thumbnail">
      </a>
      <p style="text-align: center;">Click above to view the demo video</p>
    </div> -->
    
    <!-- Abstract -->
    <h2>Abstract</h2>
    <p>
      This report details the development of a digital twin robotic manipulator built on a ROS framework. The robot has vision capabilities for various
      tasks, integrating hardware components such as an Nvidia Jetson Orin Nano, serial bus servo motor drivers, and 12V 30 kg.cm servo motors,
      the system leverages advanced computer vision algorithms and control strategies to achieve robust adaptation to environments. The system also includes
      a control panel, an IPad device, that can be used to control and generate tasks for the robot manipulator. The combined research and practical implementation 
      confirm the robot’s efficiency in adapting to the environment and handling complex tasks.
    </p>
    
    <!-- Introduction -->
    <h2>Introduction</h2>
    <p>
      Vision Language Action models is shaping modern robotic systems, particularly in applications that requires visual feedback, fast and acccurate movement, and multi-task handling.
      Traditional methods are still dominant for specialized tasks due to accurate and precise nature of operation. This project explores the use of Vision Models on simple pick and place
      tasks and building up on the results to achieve satisfactory results on more complex problems. A camera is used for depth estimation, how far the end effector is from the object of
      concern. The high-level computation, mainly the computer vision tasks, is carried out on a Nvidia Jeston Orin Nanoز
    </p>
    
    <!-- Hardware Setup -->
    <h2>Hardware Setup</h2>
    <p>
      The robot features six 12v servo motors providing 6 Degree of Freedom (DoF). The robot design was 3D printed.
      Key components include:
    </p>
    <ul>
      <li><strong>Processing:</strong> Nvidia Jetson Orin Nano running ROS2 for high-level control.</li>
      <li><strong>Control:</strong> Serial Bus Servo Motor Drivers managing six 12V servo motors.</li>
      <li><strong>Sensors:</strong> Raspberry Pi HQ camera for 3D vision</li>
      <li><strong>Display and Command:</strong> IPad Air 1 as the command and control device (any device can work)</li>
    </ul>
    
    <!-- Image Grid for Hardware -->
<div class="image-grid">
      <img src="assets/images/projects/4wd_project/robot.jpg" alt="Robot Front View">
      <img src="assets/images/projects/4wd_project/robot3.jpg" alt="Robot in Action">

      <img src="assets/images/projects/4wd_project/drive.gif" alt="Robot Side View">
      <img src="assets/images/projects/4wd_project/robot2.jpg" alt="Robot in Action">
      
      <img src="assets/images/projects/4wd_project/motortest.gif" alt="Robot in Action">
      <img src="assets/images/projects/4wd_project/body.png" alt="Robot Top View">
</div>

<style>
      .image-grid img {
            width: 100%;
            height: auto;
            object-fit: cover;
            border-radius: 8px;
      }
</style>
    
    <!-- Simulation & Software Setup -->
    <h2>Simulation and Software Setup</h2>
    <p>
      The robot was modeled in Fusion360 and converted to URDF using Fusion2URDF.
      Gazebo plugins for differential drive, odometry, and LaserScan were integrated to simulate the robot in a custom environment.
      The software stack includes ROS packages such as move_base, gmapping, and RTAB-Map, which work in tandem with sensor inputs to generate accurate maps and enable autonomous navigation.
      Installation involves cloning the repository, building the workspace with <code>catkin_make</code>, and resolving dependencies with <code>rosdep</code>.
    </p>
    
    <!-- Methodology -->
    <h2>Methodology</h2>
    <p>
      Our approach is twofold:
    </p>
    <ul>
      <li>
        <strong>Simulation Model:</strong> A detailed CAD model was developed and tested in Gazebo. The navigation stack utilizes move_base and SLAM algorithms to continuously update the map and compute optimal paths.
      </li>
      <li>
        <strong>Hardware Integration:</strong> The robot fuses data from LIDAR, Kinect, and wheel encoders to improve localization accuracy.
        An Intel RealSense T265 Tracking Camera further refines the odometry, enabling robust real-world navigation.
      </li>
    </ul>
    
    <!-- Results and Discussion -->
    <h2>Results and Discussion</h2>
    <p>
      Simulation tests in Gazebo validated the system's ability to generate both 2D and 3D maps.
      In real-world trials, the robot effectively navigated dynamic outdoor environments, demonstrating reliable obstacle avoidance and accurate localization.
      The combination of GMapping for rapid 2D mapping and RTAB-Map for richer 3D spatial awareness provides a comprehensive solution for autonomous navigation in GPS-restricted areas.
    </p>
    <div class="image-grid">
      <img src="assets/images/projects/4wd_project/sim.png" alt="Simulation in Gazebo">
      <img src="assets/images/projects/4wd_project/rtab.png" alt="RTAB-Map Output">
    </div>
    
    <!-- Conclusion -->
    <h2>Conclusion</h2>
    <p>
      The 4WD Outdoor Navigation Robot integrates advanced hardware and software to achieve robust autonomous navigation.
      By combining detailed simulation with rigorous hardware integration, the robot reliably maps its environment and navigates complex terrains.
      Future work will focus on optimizing sensor fusion, reducing computational overhead, and extending the operational range to further enhance system performance.
    </p>
    
  
    <!-- GitHub Documentation Button -->
    <div class="media-link">
      <a href="https://github.com/jerinpeter/4wdNavbot" target="_blank">
        <i class="fab fa-github" style="font-size: 2rem; vertical-align: middle; margin-right: 0.5rem;"></i> View Full Documentation on GitHub
      </a>
    </div>
  </div>
  
  <!-- Footer -->
  <footer style="text-align: center; padding: 1rem 0; background-color: #000; color: #1db954;">
    <p>&copy; 2026 Mohammed Ashraf. All rights reserved.</p>
  </footer>
</body>
</html>
